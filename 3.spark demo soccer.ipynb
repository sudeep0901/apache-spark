{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Analyzing Soccer Players\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://sudeep:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd0a8041470>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = spark.read.format('csv').option('header',\"true\").load(\"/home/sudeep/sources/github/apache spark/spark getting started/02/demos/datasets/player.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, player_api_id: string, player_name: string, player_fifa_api_id: string, birthday: string, height: string, weight: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- player_api_id: string (nullable = true)\n",
      " |-- player_name: string (nullable = true)\n",
      " |-- player_fifa_api_id: string (nullable = true)\n",
      " |-- birthday: string (nullable = true)\n",
      " |-- height: string (nullable = true)\n",
      " |-- weight: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "players.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+--------------------+------------------+-------------------+------+------+\n",
      "| id|player_api_id|         player_name|player_fifa_api_id|           birthday|height|weight|\n",
      "+---+-------------+--------------------+------------------+-------------------+------+------+\n",
      "|  1|       505942|  Aaron Appindangoye|            218353|1992-02-29 00:00:00|182.88|   187|\n",
      "|  2|       155782|     Aaron Cresswell|            189615|1989-12-15 00:00:00|170.18|   146|\n",
      "|  3|       162549|         Aaron Doran|            186170|1991-05-13 00:00:00|170.18|   163|\n",
      "|  4|        30572|       Aaron Galindo|            140161|1982-05-08 00:00:00|182.88|   198|\n",
      "|  5|        23780|        Aaron Hughes|             17725|1979-11-08 00:00:00|182.88|   154|\n",
      "|  6|        27316|          Aaron Hunt|            158138|1986-09-04 00:00:00|182.88|   161|\n",
      "|  7|       564793|          Aaron Kuhl|            221280|1996-01-30 00:00:00|172.72|   146|\n",
      "|  8|        30895|        Aaron Lennon|            152747|1987-04-16 00:00:00| 165.1|   139|\n",
      "|  9|       528212|        Aaron Lennox|            206592|1993-02-19 00:00:00| 190.5|   181|\n",
      "| 10|       101042|       Aaron Meijers|            188621|1987-10-28 00:00:00|175.26|   170|\n",
      "| 11|        23889|       Aaron Mokoena|             47189|1980-11-25 00:00:00|182.88|   181|\n",
      "| 12|       231592|          Aaron Mooy|            194958|1990-09-15 00:00:00|175.26|   150|\n",
      "| 13|       163222|      Aaron Muirhead|            213568|1990-08-30 00:00:00|187.96|   168|\n",
      "| 14|        40719|        Aaron Niguez|            183853|1989-04-26 00:00:00|170.18|   143|\n",
      "| 15|        75489|        Aaron Ramsey|            186561|1990-12-26 00:00:00| 177.8|   154|\n",
      "| 16|       597948|       Aaron Splaine|            226014|1996-10-13 00:00:00|172.72|   163|\n",
      "| 17|       161644|Aaron Taylor-Sinc...|            213569|1991-04-08 00:00:00|182.88|   176|\n",
      "| 18|        23499|     Aaron Wilbraham|              2335|1979-10-21 00:00:00| 190.5|   159|\n",
      "| 19|       120919|   Aatif Chahechouhe|            187939|1986-07-02 00:00:00|175.26|   150|\n",
      "| 20|        46447|           Abasse Ba|            156626|1976-07-12 00:00:00|187.96|   185|\n",
      "+---+-------------+--------------------+------------------+-------------------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "players.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id='1', player_api_id='505942', player_name='Aaron Appindangoye', player_fifa_api_id='218353', birthday='1992-02-29 00:00:00', height='182.88', weight='187')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'id'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "| 20|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "players.select(players['id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_attributes = spark.read.format('csv').option('header',\"true\").load(\"/home/sudeep/sources/github/apache spark/spark getting started/02/demos/datasets/player_attributes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- player_fifa_api_id: string (nullable = true)\n",
      " |-- player_api_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- overall_rating: string (nullable = true)\n",
      " |-- potential: string (nullable = true)\n",
      " |-- preferred_foot: string (nullable = true)\n",
      " |-- attacking_work_rate: string (nullable = true)\n",
      " |-- defensive_work_rate: string (nullable = true)\n",
      " |-- crossing: string (nullable = true)\n",
      " |-- finishing: string (nullable = true)\n",
      " |-- heading_accuracy: string (nullable = true)\n",
      " |-- short_passing: string (nullable = true)\n",
      " |-- volleys: string (nullable = true)\n",
      " |-- dribbling: string (nullable = true)\n",
      " |-- curve: string (nullable = true)\n",
      " |-- free_kick_accuracy: string (nullable = true)\n",
      " |-- long_passing: string (nullable = true)\n",
      " |-- ball_control: string (nullable = true)\n",
      " |-- acceleration: string (nullable = true)\n",
      " |-- sprint_speed: string (nullable = true)\n",
      " |-- agility: string (nullable = true)\n",
      " |-- reactions: string (nullable = true)\n",
      " |-- balance: string (nullable = true)\n",
      " |-- shot_power: string (nullable = true)\n",
      " |-- jumping: string (nullable = true)\n",
      " |-- stamina: string (nullable = true)\n",
      " |-- strength: string (nullable = true)\n",
      " |-- long_shots: string (nullable = true)\n",
      " |-- aggression: string (nullable = true)\n",
      " |-- interceptions: string (nullable = true)\n",
      " |-- positioning: string (nullable = true)\n",
      " |-- vision: string (nullable = true)\n",
      " |-- penalties: string (nullable = true)\n",
      " |-- marking: string (nullable = true)\n",
      " |-- standing_tackle: string (nullable = true)\n",
      " |-- sliding_tackle: string (nullable = true)\n",
      " |-- gk_diving: string (nullable = true)\n",
      " |-- gk_handling: string (nullable = true)\n",
      " |-- gk_kicking: string (nullable = true)\n",
      " |-- gk_positioning: string (nullable = true)\n",
      " |-- gk_reflexes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "players_attributes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11060, 183978)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    players.count(), players_attributes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11060"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players_attributes.select('player_api_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'player_api_id',\n",
       " 'player_name',\n",
       " 'player_fifa_api_id',\n",
       " 'birthday',\n",
       " 'height',\n",
       " 'weight']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players.drop('id', 'player_fifa_api_id')\n",
    "players.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, player_api_id: string, player_name: string, player_fifa_api_id: string, birthday: string, height: string, weight: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, player_fifa_api_id: string, player_api_id: string, date: string, overall_rating: string, potential: string, preferred_foot: string, attacking_work_rate: string, defensive_work_rate: string, crossing: string, finishing: string, heading_accuracy: string, short_passing: string, volleys: string, dribbling: string, curve: string, free_kick_accuracy: string, long_passing: string, ball_control: string, acceleration: string, sprint_speed: string, agility: string, reactions: string, balance: string, shot_power: string, jumping: string, stamina: string, strength: string, long_shots: string, aggression: string, interceptions: string, positioning: string, vision: string, penalties: string, marking: string, standing_tackle: string, sliding_tackle: string, gk_diving: string, gk_handling: string, gk_kicking: string, gk_positioning: string, gk_reflexes: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players_attributes.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11060, 183978)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    players.count(), players_attributes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_extract_udf = udf(lambda date: date.split('-')[0])\n",
    "players_attributes = players_attributes.withColumn(\"year\", year_extract_udf(players_attributes.date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-------------+-------------------+--------------+---------+--------------+-------------------+-------------------+--------+---------+----------------+-------------+-------+---------+-----+------------------+------------+------------+------------+------------+-------+---------+-------+----------+-------+-------+--------+----------+----------+-------------+-----------+------+---------+-------+---------------+--------------+---------+-----------+----------+--------------+-----------+----+\n",
      "| id|player_fifa_api_id|player_api_id|               date|overall_rating|potential|preferred_foot|attacking_work_rate|defensive_work_rate|crossing|finishing|heading_accuracy|short_passing|volleys|dribbling|curve|free_kick_accuracy|long_passing|ball_control|acceleration|sprint_speed|agility|reactions|balance|shot_power|jumping|stamina|strength|long_shots|aggression|interceptions|positioning|vision|penalties|marking|standing_tackle|sliding_tackle|gk_diving|gk_handling|gk_kicking|gk_positioning|gk_reflexes|year|\n",
      "+---+------------------+-------------+-------------------+--------------+---------+--------------+-------------------+-------------------+--------+---------+----------------+-------------+-------+---------+-----+------------------+------------+------------+------------+------------+-------+---------+-------+----------+-------+-------+--------+----------+----------+-------------+-----------+------+---------+-------+---------------+--------------+---------+-----------+----------+--------------+-----------+----+\n",
      "|  1|            218353|       505942|2016-02-18 00:00:00|            67|       71|         right|             medium|             medium|      49|       44|              71|           61|     44|       51|   45|                39|          64|          49|          60|          64|     59|       47|     65|        55|     58|     54|      76|        35|        71|           70|         45|    54|       48|     65|             69|            69|        6|         11|        10|             8|          8|2016|\n",
      "|  2|            218353|       505942|2015-11-19 00:00:00|            67|       71|         right|             medium|             medium|      49|       44|              71|           61|     44|       51|   45|                39|          64|          49|          60|          64|     59|       47|     65|        55|     58|     54|      76|        35|        71|           70|         45|    54|       48|     65|             69|            69|        6|         11|        10|             8|          8|2015|\n",
      "|  3|            218353|       505942|2015-09-21 00:00:00|            62|       66|         right|             medium|             medium|      49|       44|              71|           61|     44|       51|   45|                39|          64|          49|          60|          64|     59|       47|     65|        55|     58|     54|      76|        35|        63|           41|         45|    54|       48|     65|             66|            69|        6|         11|        10|             8|          8|2015|\n",
      "|  4|            218353|       505942|2015-03-20 00:00:00|            61|       65|         right|             medium|             medium|      48|       43|              70|           60|     43|       50|   44|                38|          63|          48|          60|          64|     59|       46|     65|        54|     58|     54|      76|        34|        62|           40|         44|    53|       47|     62|             63|            66|        5|         10|         9|             7|          7|2015|\n",
      "|  5|            218353|       505942|2007-02-22 00:00:00|            61|       65|         right|             medium|             medium|      48|       43|              70|           60|     43|       50|   44|                38|          63|          48|          60|          64|     59|       46|     65|        54|     58|     54|      76|        34|        62|           40|         44|    53|       47|     62|             63|            66|        5|         10|         9|             7|          7|2007|\n",
      "|  6|            189615|       155782|2016-04-21 00:00:00|            74|       76|          left|               high|             medium|      80|       53|              58|           71|     40|       73|   70|                69|          68|          71|          79|          78|     78|       67|     90|        71|     85|     79|      56|        62|        68|           67|         60|    66|       59|     76|             75|            78|       14|          7|         9|             9|         12|2016|\n",
      "|  7|            189615|       155782|2016-04-07 00:00:00|            74|       76|          left|               high|             medium|      80|       53|              58|           71|     32|       73|   70|                69|          68|          71|          79|          78|     78|       67|     90|        71|     85|     79|      56|        60|        68|           67|         60|    66|       59|     76|             75|            78|       14|          7|         9|             9|         12|2016|\n",
      "|  8|            189615|       155782|2016-01-07 00:00:00|            73|       75|          left|               high|             medium|      79|       52|              57|           70|     29|       71|   68|                69|          68|          70|          79|          78|     78|       67|     90|        71|     84|     79|      56|        59|        67|           66|         58|    65|       59|     76|             75|            78|       14|          7|         9|             9|         12|2016|\n",
      "|  9|            189615|       155782|2015-12-24 00:00:00|            73|       75|          left|               high|             medium|      79|       51|              57|           70|     29|       71|   68|                69|          68|          70|          79|          78|     78|       67|     90|        71|     84|     79|      56|        58|        67|           66|         58|    65|       59|     76|             75|            78|       14|          7|         9|             9|         12|2015|\n",
      "| 10|            189615|       155782|2015-12-17 00:00:00|            73|       75|          left|               high|             medium|      79|       51|              57|           70|     29|       71|   68|                69|          68|          70|          79|          78|     78|       67|     90|        71|     84|     79|      56|        58|        67|           66|         58|    65|       59|     76|             75|            78|       14|          7|         9|             9|         12|2015|\n",
      "| 11|            189615|       155782|2015-10-16 00:00:00|            73|       77|          left|               high|             medium|      79|       51|              57|           70|     29|       71|   68|                69|          68|          70|          79|          78|     78|       67|     90|        71|     84|     79|      56|        58|        67|           66|         58|    65|       59|     76|             75|            78|       14|          7|         9|             9|         12|2015|\n",
      "| 12|            189615|       155782|2015-09-25 00:00:00|            74|       78|          left|               high|             medium|      79|       51|              57|           70|     29|       71|   68|                69|          68|          70|          80|          78|     78|       67|     90|        71|     84|     79|      56|        58|        67|           66|         58|    65|       59|     76|             75|            78|       14|          7|         9|             9|         12|2015|\n",
      "| 13|            189615|       155782|2015-09-21 00:00:00|            73|       77|          left|             medium|             medium|      79|       51|              57|           70|     29|       67|   68|                69|          68|          68|          79|          78|     78|       67|     90|        71|     84|     79|      56|        58|        67|           66|         58|    65|       59|     76|             75|            78|       14|          7|         9|             9|         12|2015|\n",
      "| 14|            189615|       155782|2015-01-09 00:00:00|            71|       75|          left|             medium|             medium|      78|       50|              56|           69|     28|       66|   67|                68|          67|          67|          79|          82|     79|       71|     90|        70|     84|     79|      50|        56|        66|           65|         57|    64|       58|     73|             72|            72|       13|          6|         8|             8|         11|2015|\n",
      "| 15|            189615|       155782|2014-12-05 00:00:00|            71|       77|          left|             medium|             medium|      78|       50|              56|           69|     28|       66|   67|                68|          67|          67|          79|          82|     79|       71|     90|        70|     84|     79|      50|        56|        66|           65|         57|    64|       58|     73|             72|            72|       13|          6|         8|             8|         11|2014|\n",
      "| 16|            189615|       155782|2014-11-07 00:00:00|            71|       77|          left|             medium|             medium|      78|       50|              56|           69|     28|       66|   67|                68|          67|          67|          79|          82|     79|       71|     90|        70|     84|     79|      50|        56|        66|           65|         57|    64|       58|     73|             72|            72|       13|          6|         8|             8|         11|2014|\n",
      "| 17|            189615|       155782|2014-09-18 00:00:00|            70|       77|          left|             medium|             medium|      77|       50|              51|           67|     28|       66|   67|                68|          67|          66|          79|          82|     79|       69|     90|        70|     84|     79|      50|        56|        66|           62|         57|    64|       58|     73|             72|            72|       13|          6|         8|             8|         11|2014|\n",
      "| 18|            189615|       155782|2014-05-02 00:00:00|            70|       79|          left|             medium|             medium|      77|       50|              51|           67|     28|       66|   67|                68|          67|          66|          84|          82|     81|       69|     90|        70|     84|     80|      50|        56|        66|           62|         57|    64|       58|     73|             72|            72|       13|          6|         8|             8|         11|2014|\n",
      "| 19|            189615|       155782|2014-04-04 00:00:00|            70|       79|          left|             medium|             medium|      77|       50|              51|           67|     28|       66|   67|                68|          67|          66|          84|          82|     81|       69|     90|        70|     84|     80|      50|        56|        66|           62|         57|    64|       58|     73|             72|            72|       13|          6|         8|             8|         11|2014|\n",
      "| 20|            189615|       155782|2014-03-14 00:00:00|            70|       79|          left|             medium|             medium|      77|       50|              51|           67|     28|       66|   66|                68|          67|          65|          84|          82|     81|       69|     90|        70|     84|     79|      49|        55|        66|           62|         57|    61|       58|     73|             72|            72|       13|          6|         8|             8|         11|2014|\n",
      "+---+------------------+-------------+-------------------+--------------+---------+--------------+-------------------+-------------------+--------+---------+----------------+-------------+-------+---------+-----+------------------+------------+------------+------------+------------+-------+---------+-------+----------+-------+-------+--------+----------+----------+-------------+-----------+------+---------+-------+---------------+--------------+---------+-----------+----------+--------------+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "players_attributes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'date'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players_attributes.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, player_fifa_api_id: string, player_api_id: string, overall_rating: string, potential: string, preferred_foot: string, attacking_work_rate: string, defensive_work_rate: string, crossing: string, finishing: string, heading_accuracy: string, short_passing: string, volleys: string, dribbling: string, curve: string, free_kick_accuracy: string, long_passing: string, ball_control: string, acceleration: string, sprint_speed: string, agility: string, reactions: string, balance: string, shot_power: string, jumping: string, stamina: string, strength: string, long_shots: string, aggression: string, interceptions: string, positioning: string, vision: string, penalties: string, marking: string, standing_tackle: string, sliding_tackle: string, gk_diving: string, gk_handling: string, gk_kicking: string, gk_positioning: string, gk_reflexes: string, year: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players_attributes.drop(players_attributes.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'player_fifa_api_id',\n",
       " 'player_api_id',\n",
       " 'date',\n",
       " 'overall_rating',\n",
       " 'potential',\n",
       " 'preferred_foot',\n",
       " 'attacking_work_rate',\n",
       " 'defensive_work_rate',\n",
       " 'crossing',\n",
       " 'finishing',\n",
       " 'heading_accuracy',\n",
       " 'short_passing',\n",
       " 'volleys',\n",
       " 'dribbling',\n",
       " 'curve',\n",
       " 'free_kick_accuracy',\n",
       " 'long_passing',\n",
       " 'ball_control',\n",
       " 'acceleration',\n",
       " 'sprint_speed',\n",
       " 'agility',\n",
       " 'reactions',\n",
       " 'balance',\n",
       " 'shot_power',\n",
       " 'jumping',\n",
       " 'stamina',\n",
       " 'strength',\n",
       " 'long_shots',\n",
       " 'aggression',\n",
       " 'interceptions',\n",
       " 'positioning',\n",
       " 'vision',\n",
       " 'penalties',\n",
       " 'marking',\n",
       " 'standing_tackle',\n",
       " 'sliding_tackle',\n",
       " 'gk_diving',\n",
       " 'gk_handling',\n",
       " 'gk_kicking',\n",
       " 'gk_positioning',\n",
       " 'gk_reflexes',\n",
       " 'year']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players_attributes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best striker\n",
    "pa_2016 = players_attributes.filter(players_attributes.year == 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14103"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_2016.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5586"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_2016.select(pa_2016.player_api_id).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_striker_2016 =  pa_2016.groupBy('player_api_id').agg({'finishing':'avg',\n",
    "                                                        'shot_power': 'avg',\n",
    "                                                       'acceleration':'avg'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+------------------+------------------+\n",
      "|player_api_id|   avg(finishing)| avg(acceleration)|   avg(shot_power)|\n",
      "+-------------+-----------------+------------------+------------------+\n",
      "|       309726|75.44444444444444| 74.11111111111111|              76.0|\n",
      "|        26112|             53.0|              51.0|              76.0|\n",
      "|        38433|            68.25|              74.0|              74.0|\n",
      "|       295060|             25.0|              62.0|              40.0|\n",
      "|       161396|             29.0|              72.0|              69.0|\n",
      "|        37774|             61.0|              64.0|              68.0|\n",
      "|        41157|             81.0|              87.0|              80.0|\n",
      "|        40740|             58.0|              73.5|              75.0|\n",
      "|        31432|             14.0|              59.0|              65.0|\n",
      "|       109653|             62.0|              65.0|              83.5|\n",
      "|       282680|             12.0|              33.0|              24.0|\n",
      "|       210428|             19.0|              69.0|              55.0|\n",
      "|       190851|             67.0|              74.0|              78.0|\n",
      "|       419238|             23.0|57.333333333333336|55.333333333333336|\n",
      "|       664587|             58.0|              74.0|              61.0|\n",
      "|       239352|             63.0|              83.0|              64.0|\n",
      "|       190801|             63.0|             63.75|              56.5|\n",
      "|       196957|             66.0|              76.0| 75.66666666666667|\n",
      "|       173922|             65.0|              78.0|              66.0|\n",
      "|       121080|             36.0|              72.0|              39.0|\n",
      "+-------------+-----------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pa_striker_2016.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_striker_2016  = pa_striker_2016.withColumnRenamed(\"avg(finishing)\",\"finishing\")\\\n",
    "                                  .withColumnRenamed(\"avg(acceleration)\",\"acceleration\")\\\n",
    "                                  .withColumnRenamed(\"avg(shot_power)\",\"shot_power\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+------------------+------------------+\n",
      "|player_api_id|        finishing|      acceleration|        shot_power|\n",
      "+-------------+-----------------+------------------+------------------+\n",
      "|       309726|75.44444444444444| 74.11111111111111|              76.0|\n",
      "|        26112|             53.0|              51.0|              76.0|\n",
      "|        38433|            68.25|              74.0|              74.0|\n",
      "|       295060|             25.0|              62.0|              40.0|\n",
      "|       161396|             29.0|              72.0|              69.0|\n",
      "|        37774|             61.0|              64.0|              68.0|\n",
      "|        41157|             81.0|              87.0|              80.0|\n",
      "|        40740|             58.0|              73.5|              75.0|\n",
      "|        31432|             14.0|              59.0|              65.0|\n",
      "|       109653|             62.0|              65.0|              83.5|\n",
      "|       282680|             12.0|              33.0|              24.0|\n",
      "|       210428|             19.0|              69.0|              55.0|\n",
      "|       190851|             67.0|              74.0|              78.0|\n",
      "|       419238|             23.0|57.333333333333336|55.333333333333336|\n",
      "|       664587|             58.0|              74.0|              61.0|\n",
      "|       239352|             63.0|              83.0|              64.0|\n",
      "|       190801|             63.0|             63.75|              56.5|\n",
      "|       196957|             66.0|              76.0| 75.66666666666667|\n",
      "|       173922|             65.0|              78.0|              66.0|\n",
      "|       121080|             36.0|              72.0|              39.0|\n",
      "+-------------+-----------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pa_striker_2016.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_finishing = 1\n",
    "weight_shot_power = 2\n",
    "weight_acceleration = 1\n",
    "\n",
    "total_weight = weight_finishing + weight_shot_power + weight_acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "strikers = pa_striker_2016.withColumn('striker_grade',\n",
    "                                     (pa_striker_2016.finishing * weight_finishing +\n",
    "                                     pa_striker_2016.shot_power * weight_shot_power +\n",
    "                                     pa_striker_2016.acceleration * weight_acceleration * weight_acceleration) / total_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+------------------+------------------+-----------------+\n",
      "|player_api_id|        finishing|      acceleration|        shot_power|    striker_grade|\n",
      "+-------------+-----------------+------------------+------------------+-----------------+\n",
      "|       309726|75.44444444444444| 74.11111111111111|              76.0|75.38888888888889|\n",
      "|        26112|             53.0|              51.0|              76.0|             64.0|\n",
      "|        38433|            68.25|              74.0|              74.0|          72.5625|\n",
      "|       295060|             25.0|              62.0|              40.0|            41.75|\n",
      "|       161396|             29.0|              72.0|              69.0|            59.75|\n",
      "|        37774|             61.0|              64.0|              68.0|            65.25|\n",
      "|        41157|             81.0|              87.0|              80.0|             82.0|\n",
      "|        40740|             58.0|              73.5|              75.0|           70.375|\n",
      "|        31432|             14.0|              59.0|              65.0|            50.75|\n",
      "|       109653|             62.0|              65.0|              83.5|             73.5|\n",
      "|       282680|             12.0|              33.0|              24.0|            23.25|\n",
      "|       210428|             19.0|              69.0|              55.0|             49.5|\n",
      "|       190851|             67.0|              74.0|              78.0|            74.25|\n",
      "|       419238|             23.0|57.333333333333336|55.333333333333336|47.75000000000001|\n",
      "|       664587|             58.0|              74.0|              61.0|             63.5|\n",
      "|       239352|             63.0|              83.0|              64.0|             68.5|\n",
      "|       190801|             63.0|             63.75|              56.5|          59.9375|\n",
      "|       196957|             66.0|              76.0| 75.66666666666667|73.33333333333334|\n",
      "|       173922|             65.0|              78.0|              66.0|            68.75|\n",
      "|       121080|             36.0|              72.0|              39.0|             46.5|\n",
      "+-------------+-----------------+------------------+------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "strikers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[player_api_id: string, finishing: double, acceleration: double, shot_power: double, striker_grade: double]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strikers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "strikers = strikers.filter(strikers.striker_grade >  70).sort(strikers.striker_grade.desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+------------+-----------------+-----------------+\n",
      "|player_api_id|        finishing|acceleration|       shot_power|    striker_grade|\n",
      "+-------------+-----------------+------------+-----------------+-----------------+\n",
      "|        20276|             85.0|        84.0|             94.0|            89.25|\n",
      "|        37412|             90.0|        92.0|             87.0|             89.0|\n",
      "|        38817|             88.0|        90.0|             88.5|            88.75|\n",
      "|        32118|             85.0|        82.0|             93.0|            88.25|\n",
      "|        31921|             81.0|        93.0|             87.0|             87.0|\n",
      "|        30834|             85.0|        90.0|             86.0|            86.75|\n",
      "|       303824|73.42857142857143|        91.0|             88.0|85.10714285714286|\n",
      "|       129944|             83.0|        89.0|             84.0|             85.0|\n",
      "|       158263|             77.0|        90.0|             86.0|            84.75|\n",
      "|       150565|             88.0|        95.0|             78.0|            84.75|\n",
      "|        25759|89.66666666666667|        79.0|             85.0|84.66666666666667|\n",
      "|       156726|             72.0|        96.0|             85.0|             84.5|\n",
      "|       169193|             86.0|       87.25|            82.25|          84.4375|\n",
      "|       286119|79.85714285714286|        91.0|83.42857142857143|84.42857142857143|\n",
      "|        30348|             83.5|        78.0|             88.0|           84.375|\n",
      "|        93447|             89.0|        79.0|             84.5|            84.25|\n",
      "|        46509|             75.0|        88.0|             87.0|            84.25|\n",
      "|        50047|            84.25|       88.75|             82.0|            84.25|\n",
      "|       178812|             75.0|        91.0|             85.0|             84.0|\n",
      "|       181276|             84.0|        80.0|             86.0|             84.0|\n",
      "+-------------+-----------------+------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "strikers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1609, 11060)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strikers.count(), players.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "striker_details = players.join(strikers, players.player_api_id == strikers.player_api_id) #inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+--------------------+------------------+-------------------+------+------+-------------+-----------------+------------+-----------------+-----------------+\n",
      "|  id|player_api_id|         player_name|player_fifa_api_id|           birthday|height|weight|player_api_id|        finishing|acceleration|       shot_power|    striker_grade|\n",
      "+----+-------------+--------------------+------------------+-------------------+------+------+-------------+-----------------+------------+-----------------+-----------------+\n",
      "|4283|        20276|                Hulk|            189362|1986-07-25 00:00:00|180.34|   187|        20276|             85.0|        84.0|             94.0|            89.25|\n",
      "|9674|        37412|       Sergio Aguero|            153079|1988-06-02 00:00:00|172.72|   163|        37412|             90.0|        92.0|             87.0|             89.0|\n",
      "|1581|        38817|        Carlos Tevez|            143001|1984-02-05 00:00:00|172.72|   157|        38817|             88.0|        90.0|             88.5|            88.75|\n",
      "|6400|        32118|      Lukas Podolski|            150516|1985-06-04 00:00:00|182.88|   183|        32118|             85.0|        82.0|             93.0|            88.25|\n",
      "|3660|        31921|         Gareth Bale|            173731|1989-07-16 00:00:00|182.88|   163|        31921|             81.0|        93.0|             87.0|             87.0|\n",
      "| 951|        30834|        Arjen Robben|              9014|1984-01-23 00:00:00|180.34|   176|        30834|             85.0|        90.0|             86.0|            86.75|\n",
      "|7317|       303824|       Memphis Depay|            202556|1994-02-13 00:00:00|175.26|   172|       303824|73.42857142857143|        91.0|             88.0|85.10714285714286|\n",
      "|6725|       129944|          Marco Reus|            188350|1989-05-31 00:00:00|180.34|   165|       129944|             83.0|        89.0|             84.0|             85.0|\n",
      "|2772|       158263|        Dorlan Pabon|            196143|1988-01-24 00:00:00|167.64|   161|       158263|             77.0|        90.0|             86.0|            84.75|\n",
      "|8625|       150565|Pierre-Emerick Au...|            188567|1989-06-18 00:00:00|187.96|   176|       150565|             88.0|        95.0|             78.0|            84.75|\n",
      "|3936|        25759|     Gonzalo Higuain|            167664|1987-12-10 00:00:00|182.88|   181|        25759|89.66666666666667|        79.0|             85.0|84.66666666666667|\n",
      "|2780|       156726|       Douglas Costa|            190483|1990-09-14 00:00:00|172.72|   143|       156726|             72.0|        96.0|             85.0|             84.5|\n",
      "| 482|       169193| Alexandre Lacazette|            193301|1991-05-28 00:00:00|175.26|   161|       169193|             86.0|       87.25|            82.25|          84.4375|\n",
      "|4623|       286119|         Jamie Vardy|            208830|1987-01-11 00:00:00| 177.8|   161|       286119|79.85714285714286|        91.0|83.42857142857143|84.42857142857143|\n",
      "|4871|        30348|       Jermain Defoe|             50542|1982-10-07 00:00:00|170.18|   154|        30348|             83.5|        78.0|             88.0|           84.375|\n",
      "| 797|        46509|      Angel Di Maria|            183898|1988-02-14 00:00:00|180.34|   165|        46509|             75.0|        88.0|             87.0|            84.25|\n",
      "|9039|        93447|  Robert Lewandowski|            188545|1988-08-21 00:00:00|185.42|   174|        93447|             89.0|        79.0|             84.5|            84.25|\n",
      "| 501|        50047|      Alexis Sanchez|            184941|1988-12-19 00:00:00|170.18|   137|        50047|            84.25|       88.75|             82.0|            84.25|\n",
      "|9202|       181276|       Romelu Lukaku|            192505|1993-05-13 00:00:00| 190.5|   207|       181276|             84.0|        80.0|             86.0|             84.0|\n",
      "| 670|       178812|     Andre Schuerrle|            193130|1990-11-06 00:00:00|182.88|   163|       178812|             75.0|        91.0|             85.0|             84.0|\n",
      "+----+-------------+--------------------+------------------+-------------------+------+------+-------------+-----------------+------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "striker_details.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'player_api_id',\n",
       " 'player_name',\n",
       " 'player_fifa_api_id',\n",
       " 'birthday',\n",
       " 'height',\n",
       " 'weight',\n",
       " 'player_api_id',\n",
       " 'finishing',\n",
       " 'acceleration',\n",
       " 'shot_power',\n",
       " 'striker_grade']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "striker_details.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1609"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "striker_details.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different syntax \n",
    "striker_details = players.join(strikers, ['player_api_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+--------------+------------------+-------------------+------+------+-------------+\n",
      "|player_api_id|  id|   player_name|player_fifa_api_id|           birthday|height|weight|striker_grade|\n",
      "+-------------+----+--------------+------------------+-------------------+------+------+-------------+\n",
      "|        20276|4283|          Hulk|            189362|1986-07-25 00:00:00|180.34|   187|        89.25|\n",
      "|        37412|9674| Sergio Aguero|            153079|1988-06-02 00:00:00|172.72|   163|         89.0|\n",
      "|        38817|1581|  Carlos Tevez|            143001|1984-02-05 00:00:00|172.72|   157|        88.75|\n",
      "|        32118|6400|Lukas Podolski|            150516|1985-06-04 00:00:00|182.88|   183|        88.25|\n",
      "|        31921|3660|   Gareth Bale|            173731|1989-07-16 00:00:00|182.88|   163|         87.0|\n",
      "+-------------+----+--------------+------------------+-------------------+------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "striker_details = striker_details.drop('finishing')\n",
    "striker_details =striker_details.drop('acceleration')\n",
    "striker_details =striker_details.drop('shot_power')\n",
    "\n",
    "\n",
    "\n",
    "striker_details.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heavy dataset joing use brodcast variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "striker_details = players.select('player_api_id', 'player_name').join(broadcast(strikers), ['player_api_id'], 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+---------+------------+----------+-------------+\n",
      "|player_api_id|      player_name|finishing|acceleration|shot_power|striker_grade|\n",
      "+-------------+-----------------+---------+------------+----------+-------------+\n",
      "|        27316|       Aaron Hunt|     72.0|        75.0|      76.0|        74.75|\n",
      "|        40719|     Aaron Niguez|     67.0|        82.0|      74.0|        74.25|\n",
      "|        75489|     Aaron Ramsey|     75.0|        70.5|      81.0|       76.875|\n",
      "|       120919|Aatif Chahechouhe|     76.0|        80.0|      78.0|         78.0|\n",
      "|        67334|Abdoul Karim Yoda|     70.0|        84.0|      71.0|         74.0|\n",
      "+-------------+-----------------+---------+------------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "striker_details.show(5) # broadcast smaller dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "striker_details = striker_details.sort(striker_details.striker_grade.desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+---------+------------+----------+-------------+\n",
      "|player_api_id|   player_name|finishing|acceleration|shot_power|striker_grade|\n",
      "+-------------+--------------+---------+------------+----------+-------------+\n",
      "|        20276|          Hulk|     85.0|        84.0|      94.0|        89.25|\n",
      "|        37412| Sergio Aguero|     90.0|        92.0|      87.0|         89.0|\n",
      "|        38817|  Carlos Tevez|     88.0|        90.0|      88.5|        88.75|\n",
      "|        32118|Lukas Podolski|     85.0|        82.0|      93.0|        88.25|\n",
      "|        31921|   Gareth Bale|     81.0|        93.0|      87.0|         87.0|\n",
      "+-------------+--------------+---------+------------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "striker_details.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "# Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11060, 183978)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players.count(), players_attributes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_heading_acc = players_attributes.select('player_api_id', 'heading_accuracy')\\\n",
    ".join(broadcast(players), players_attributes.player_api_id == players.player_api_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+---+-------------+------------------+------------------+-------------------+------+------+\n",
      "|player_api_id|heading_accuracy| id|player_api_id|       player_name|player_fifa_api_id|           birthday|height|weight|\n",
      "+-------------+----------------+---+-------------+------------------+------------------+-------------------+------+------+\n",
      "|       505942|              71|  1|       505942|Aaron Appindangoye|            218353|1992-02-29 00:00:00|182.88|   187|\n",
      "|       505942|              71|  1|       505942|Aaron Appindangoye|            218353|1992-02-29 00:00:00|182.88|   187|\n",
      "|       505942|              71|  1|       505942|Aaron Appindangoye|            218353|1992-02-29 00:00:00|182.88|   187|\n",
      "|       505942|              70|  1|       505942|Aaron Appindangoye|            218353|1992-02-29 00:00:00|182.88|   187|\n",
      "|       505942|              70|  1|       505942|Aaron Appindangoye|            218353|1992-02-29 00:00:00|182.88|   187|\n",
      "|       155782|              58|  2|       155782|   Aaron Cresswell|            189615|1989-12-15 00:00:00|170.18|   146|\n",
      "|       155782|              58|  2|       155782|   Aaron Cresswell|            189615|1989-12-15 00:00:00|170.18|   146|\n",
      "|       155782|              57|  2|       155782|   Aaron Cresswell|            189615|1989-12-15 00:00:00|170.18|   146|\n",
      "|       155782|              57|  2|       155782|   Aaron Cresswell|            189615|1989-12-15 00:00:00|170.18|   146|\n",
      "|       155782|              57|  2|       155782|   Aaron Cresswell|            189615|1989-12-15 00:00:00|170.18|   146|\n",
      "|       155782|              57|  2|       155782|   Aaron Cresswell|            189615|1989-12-15 00:00:00|170.18|   146|\n",
      "|       155782|              57|  2|       155782|   Aaron Cresswell|            189615|1989-12-15 00:00:00|170.18|   146|\n",
      "|       155782|              57|  2|       155782|   Aaron Cresswell|            189615|1989-12-15 00:00:00|170.18|   146|\n",
      "|       155782|              56|  2|       155782|   Aaron Cresswell|            189615|1989-12-15 00:00:00|170.18|   146|\n",
      "|       155782|              56|  2|       155782|   Aaron Cresswell|            189615|1989-12-15 00:00:00|170.18|   146|\n",
      "|       155782|              56|  2|       155782|   Aaron Cresswell|            189615|1989-12-15 00:00:00|170.18|   146|\n",
      "|       155782|              51|  2|       155782|   Aaron Cresswell|            189615|1989-12-15 00:00:00|170.18|   146|\n",
      "|       155782|              51|  2|       155782|   Aaron Cresswell|            189615|1989-12-15 00:00:00|170.18|   146|\n",
      "|       155782|              51|  2|       155782|   Aaron Cresswell|            189615|1989-12-15 00:00:00|170.18|   146|\n",
      "|       155782|              51|  2|       155782|   Aaron Cresswell|            189615|1989-12-15 00:00:00|170.18|   146|\n",
      "+-------------+----------------+---+-------------+------------------+------------------+-------------------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "players_heading_acc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['player_api_id',\n",
       " 'heading_accuracy',\n",
       " 'id',\n",
       " 'player_api_id',\n",
       " 'player_name',\n",
       " 'player_fifa_api_id',\n",
       " 'birthday',\n",
       " 'height',\n",
       " 'weight']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players_heading_acc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_count = spark.sparkContext.accumulator(0)\n",
    "medium_low_count = spark.sparkContext.accumulator(0)\n",
    "medium_high_count = spark.sparkContext.accumulator(0)\n",
    "tall_count = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_players_by_height(row):\n",
    "    height = float(row.height)\n",
    "    \n",
    "    if (height <175):\n",
    "        short_count.add(1)\n",
    "    elif (height <= 183 and height > 175):\n",
    "        medium_low_count.add(1)\n",
    "    elif (height <= 195 and height > 183):\n",
    "        medium_high_count.add(1)\n",
    "    elif (height > 195):\n",
    "        tall_count.add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_heading_acc.foreach(lambda x : count_players_by_height(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_players = [short_count.value, \n",
    "            medium_low_count.value,\n",
    "            medium_high_count.value,\n",
    "            tall_count.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Reference 'player_api_id' is ambiguous, could be: player_api_id, player_api_id.;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/program files/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o605.drop.\n: org.apache.spark.sql.AnalysisException: Reference 'player_api_id' is ambiguous, could be: player_api_id, player_api_id.;\n\tat org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:259)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveQuoted(LogicalPlan.scala:121)\n\tat org.apache.spark.sql.Dataset.resolve(Dataset.scala:221)\n\tat org.apache.spark.sql.DataFrameNaFunctions$$anonfun$1.apply(DataFrameNaFunctions.scala:125)\n\tat org.apache.spark.sql.DataFrameNaFunctions$$anonfun$1.apply(DataFrameNaFunctions.scala:125)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.DataFrameNaFunctions.drop(DataFrameNaFunctions.scala:125)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-aed46115e452>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mall_players\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplayers_heading_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mdropna\u001b[0;34m(self, how, thresh, subset)\u001b[0m\n\u001b[1;32m   1605\u001b[0m             \u001b[0mthresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'any'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1607\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1.3.1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/program files/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"Reference 'player_api_id' is ambiguous, could be: player_api_id, player_api_id.;\""
     ]
    }
   ],
   "source": [
    "all_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heading accuracy\n",
    "short_ha_count = spark.sparkContext.accumulator(0)\n",
    "medium_low_ha_count = spark.sparkContext.accumulator(0)\n",
    "medium_high_ha_count = spark.sparkContext.accumulator(0)\n",
    "tall_ha_count = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_players_by_height_heading_accuracy(row,threshold_score):\n",
    "    print(row.show())\n",
    "    return\n",
    "    height = float(row.height)\n",
    "    print(row.heading_accuracy)\n",
    "    ha = float(row.heading_accuracy)\n",
    "    \n",
    "    if ha < threshold_score:\n",
    "        return\n",
    "\n",
    "    \n",
    "    if (height <175):\n",
    "        short_ha_count.add(1)\n",
    "    elif (height <= 183 and height > 175):\n",
    "        medium_low_ha_count.add(1)\n",
    "    elif (height <= 195 and height > 183):\n",
    "        medium_high_ha_count.add(1)\n",
    "    elif (height > 195):\n",
    "        tall_ha_count.add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 133.0 failed 1 times, most recent failure: Lost task 2.0 in stage 133.0 (TID 5986, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1527, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'show' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 787, in processPartition\n    f(x)\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-125-b07319efe403>\", line 1, in <lambda>\n  File \"<ipython-input-122-cabe873c41b1>\", line 2, in count_players_by_height_heading_accuracy\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1532, in __getattr__\n    raise AttributeError(item)\nAttributeError: show\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1527, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'show' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 787, in processPartition\n    f(x)\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-125-b07319efe403>\", line 1, in <lambda>\n  File \"<ipython-input-122-cabe873c41b1>\", line 2, in count_players_by_height_heading_accuracy\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1532, in __getattr__\n    raise AttributeError(item)\nAttributeError: show\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-b07319efe403>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplayers_heading_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mcount_players_by_height_heading_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mforeach\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mforeach\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessPartition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Force evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforeachPartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \"\"\"\n\u001b[0;32m-> 1055\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \"\"\"\n\u001b[0;32m-> 1046\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/program files/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/program files/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 133.0 failed 1 times, most recent failure: Lost task 2.0 in stage 133.0 (TID 5986, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1527, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'show' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 787, in processPartition\n    f(x)\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-125-b07319efe403>\", line 1, in <lambda>\n  File \"<ipython-input-122-cabe873c41b1>\", line 2, in count_players_by_height_heading_accuracy\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1532, in __getattr__\n    raise AttributeError(item)\nAttributeError: show\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1527, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'show' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/rdd.py\", line 787, in processPartition\n    f(x)\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-125-b07319efe403>\", line 1, in <lambda>\n  File \"<ipython-input-122-cabe873c41b1>\", line 2, in count_players_by_height_heading_accuracy\n  File \"/home/sudeep/program files/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1532, in __getattr__\n    raise AttributeError(item)\nAttributeError: show\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "players_heading_acc.foreach(lambda x : count_players_by_height_heading_accuracy(x, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|heading_accuracy|\n",
      "+----------------+\n",
      "|              71|\n",
      "|              71|\n",
      "|              71|\n",
      "|              70|\n",
      "|              70|\n",
      "|              58|\n",
      "|              58|\n",
      "|              57|\n",
      "|              57|\n",
      "|              57|\n",
      "|              57|\n",
      "|              57|\n",
      "|              57|\n",
      "|              56|\n",
      "|              56|\n",
      "|              56|\n",
      "|              51|\n",
      "|              51|\n",
      "|              51|\n",
      "|              51|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- player_api_id: string (nullable = true)\n",
      " |-- heading_accuracy: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- player_api_id: string (nullable = true)\n",
      " |-- player_name: string (nullable = true)\n",
      " |-- player_fifa_api_id: string (nullable = true)\n",
      " |-- birthday: string (nullable = true)\n",
      " |-- height: string (nullable = true)\n",
      " |-- weight: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "players_heading_acc.select('heading_accuracy').show()\n",
    "players_heading_acc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'player_fifa_api_id',\n",
       " 'player_api_id',\n",
       " 'date',\n",
       " 'overall_rating',\n",
       " 'potential',\n",
       " 'preferred_foot',\n",
       " 'attacking_work_rate',\n",
       " 'defensive_work_rate',\n",
       " 'crossing',\n",
       " 'finishing',\n",
       " 'heading_accuracy',\n",
       " 'short_passing',\n",
       " 'volleys',\n",
       " 'dribbling',\n",
       " 'curve',\n",
       " 'free_kick_accuracy',\n",
       " 'long_passing',\n",
       " 'ball_control',\n",
       " 'acceleration',\n",
       " 'sprint_speed',\n",
       " 'agility',\n",
       " 'reactions',\n",
       " 'balance',\n",
       " 'shot_power',\n",
       " 'jumping',\n",
       " 'stamina',\n",
       " 'strength',\n",
       " 'long_shots',\n",
       " 'aggression',\n",
       " 'interceptions',\n",
       " 'positioning',\n",
       " 'vision',\n",
       " 'penalties',\n",
       " 'marking',\n",
       " 'standing_tackle',\n",
       " 'sliding_tackle',\n",
       " 'gk_diving',\n",
       " 'gk_handling',\n",
       " 'gk_kicking',\n",
       " 'gk_positioning',\n",
       " 'gk_reflexes',\n",
       " 'year']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving to json\n",
    "pa_2016.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_2016.select(\"player_api_id\", \"overall_rating\").coalesce(1).write.option(\"header\", \"true\").csv(\"players_overall.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_2016.select(\"player_api_id\", \"overall_rating\").write.json(\"players_overall.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
